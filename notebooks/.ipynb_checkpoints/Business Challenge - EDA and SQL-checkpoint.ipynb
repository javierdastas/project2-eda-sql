{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d6951e-6cef-494f-9a57-ed5d7fd8b2dd",
   "metadata": {},
   "source": [
    "# Project 2 Business Challenge: EDA and SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e26a9330-8f77-4d12-940a-9ba70ceda137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b929b5-ff34-4471-980b-f64f1d81b24e",
   "metadata": {},
   "source": [
    "#### Load the Datasets\n",
    "\n",
    "Let's load the three datasets (`Features`, `Sales`, and `Stores`) into Python using the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b3c48c-c790-4495-9405-72bcd9cdfdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasets_path = r\"C:\\Users\\mathl\\OneDrive\\Ironhack\\Week 3\\Week 3 - Day 4\\project-2-eda-sql-master\\Kaggle Datasets - Retail Data Analytics\\\\\"\n",
    "datasets_path = \"../datasets/\"\n",
    "\n",
    "# Define file paths\n",
    "features_file_path = f\"{datasets_path}features_dataset.csv\"\n",
    "sales_file_path = f\"{datasets_path}sales_dataset.csv\"\n",
    "stores_file_path = f\"{datasets_path}stores_dataset.csv\"\n",
    "\n",
    "# Load datasets into pandas DataFrames\n",
    "features_dataset = pd.read_csv(features_file_path)\n",
    "sales_dataset = pd.read_csv(sales_file_path)\n",
    "stores_dataset = pd.read_csv(stores_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35536552-2af5-4d2d-9e19-7cda360a9e8f",
   "metadata": {},
   "source": [
    "#### Inspect the Datasets\n",
    "After loading the datasets, we will inspect their structure to understand their contents. Specifically, we will:\n",
    "- Check the first few rows of each dataset (`.head()`).\n",
    "- Review the column names and data types (`.info()`).\n",
    "- Identify any duplicates rows to ensure data integrity.\n",
    "- Identify any missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a59f13f-28cf-42a1-9b3d-33edf4752cba",
   "metadata": {},
   "source": [
    "##### First few rows of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cd4ff06-b259-47c0-a75e-00cd9b4b9380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Dataset:\n",
      "   Store        Date  Temperature  Fuel_Price  MarkDown1  MarkDown2  \\\n",
      "0      1  05/02/2010        42.31       2.572        NaN        NaN   \n",
      "1      1  12/02/2010        38.51       2.548        NaN        NaN   \n",
      "2      1  19/02/2010        39.93       2.514        NaN        NaN   \n",
      "3      1  26/02/2010        46.63       2.561        NaN        NaN   \n",
      "4      1  05/03/2010        46.50       2.625        NaN        NaN   \n",
      "\n",
      "   MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday  \n",
      "0        NaN        NaN        NaN  211.096358         8.106      False  \n",
      "1        NaN        NaN        NaN  211.242170         8.106       True  \n",
      "2        NaN        NaN        NaN  211.289143         8.106      False  \n",
      "3        NaN        NaN        NaN  211.319643         8.106      False  \n",
      "4        NaN        NaN        NaN  211.350143         8.106      False   \n",
      "\n",
      "Sales Dataset:\n",
      "   Store  Dept        Date  Weekly_Sales  IsHoliday\n",
      "0      1     1  05/02/2010      24924.50      False\n",
      "1      1     1  12/02/2010      46039.49       True\n",
      "2      1     1  19/02/2010      41595.55      False\n",
      "3      1     1  26/02/2010      19403.54      False\n",
      "4      1     1  05/03/2010      21827.90      False \n",
      "\n",
      "Stores Dataset:\n",
      "   Store Type    Size\n",
      "0      1    A  151315\n",
      "1      2    A  202307\n",
      "2      3    B   37392\n",
      "3      4    A  205863\n",
      "4      5    B   34875\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of each dataset to ensure they are loaded correctly\n",
    "print(\"Features Dataset:\")\n",
    "print(features_dataset.head(), \"\\n\")\n",
    "\n",
    "print(\"Sales Dataset:\")\n",
    "print(sales_dataset.head(), \"\\n\")\n",
    "\n",
    "print(\"Stores Dataset:\")\n",
    "print(stores_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f3bffb-4c6c-4e31-9b53-0c64b7913623",
   "metadata": {},
   "source": [
    "##### Structure of each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a6f103a-5906-402e-913c-43038da9fa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8190 entries, 0 to 8189\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Store         8190 non-null   int64  \n",
      " 1   Date          8190 non-null   object \n",
      " 2   Temperature   8190 non-null   float64\n",
      " 3   Fuel_Price    8190 non-null   float64\n",
      " 4   MarkDown1     4032 non-null   float64\n",
      " 5   MarkDown2     2921 non-null   float64\n",
      " 6   MarkDown3     3613 non-null   float64\n",
      " 7   MarkDown4     3464 non-null   float64\n",
      " 8   MarkDown5     4050 non-null   float64\n",
      " 9   CPI           7605 non-null   float64\n",
      " 10  Unemployment  7605 non-null   float64\n",
      " 11  IsHoliday     8190 non-null   bool   \n",
      "dtypes: bool(1), float64(9), int64(1), object(1)\n",
      "memory usage: 712.0+ KB\n",
      "\n",
      "Sales Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 421570 entries, 0 to 421569\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Store         421570 non-null  int64  \n",
      " 1   Dept          421570 non-null  int64  \n",
      " 2   Date          421570 non-null  object \n",
      " 3   Weekly_Sales  421570 non-null  float64\n",
      " 4   IsHoliday     421570 non-null  bool   \n",
      "dtypes: bool(1), float64(1), int64(2), object(1)\n",
      "memory usage: 13.3+ MB\n",
      "\n",
      "Stores Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45 entries, 0 to 44\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Store   45 non-null     int64 \n",
      " 1   Type    45 non-null     object\n",
      " 2   Size    45 non-null     int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check the structure of the datasets\n",
    "print(\"Features Dataset Info:\")\n",
    "features_dataset.info()\n",
    "\n",
    "print(\"\\nSales Dataset Info:\")\n",
    "sales_dataset.info()\n",
    "\n",
    "print(\"\\nStores Dataset Info:\")\n",
    "stores_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071984d-64c6-44fc-a51b-f2372170733c",
   "metadata": {},
   "source": [
    "#### Identifying duplicate rows in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8fc5fcd-b8ea-4d06-a59e-3deda920e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate Rows in Features Dataset: 0\n",
      "Duplicate Rows in Sales Dataset: 0\n",
      "Duplicate Rows in Stores Dataset: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows in each dataset\n",
    "print(\"\\nDuplicate Rows in Features Dataset:\", features_dataset.duplicated().sum())\n",
    "print(\"Duplicate Rows in Sales Dataset:\", sales_dataset.duplicated().sum())\n",
    "print(\"Duplicate Rows in Stores Dataset:\", stores_dataset.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914032a1-902b-4174-8ca1-a0a066b57c37",
   "metadata": {},
   "source": [
    "#### Identifying missing values in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "470a5b09-550c-46c0-abb2-25c9ef396aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values in Features Dataset:\n",
      "Store              0\n",
      "Date               0\n",
      "Temperature        0\n",
      "Fuel_Price         0\n",
      "MarkDown1       4158\n",
      "MarkDown2       5269\n",
      "MarkDown3       4577\n",
      "MarkDown4       4726\n",
      "MarkDown5       4140\n",
      "CPI              585\n",
      "Unemployment     585\n",
      "IsHoliday          0\n",
      "dtype: int64\n",
      "\n",
      "Missing Values in Sales Dataset:\n",
      "Store           0\n",
      "Dept            0\n",
      "Date            0\n",
      "Weekly_Sales    0\n",
      "IsHoliday       0\n",
      "dtype: int64\n",
      "\n",
      "Missing Values in Stores Dataset:\n",
      "Store    0\n",
      "Type     0\n",
      "Size     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nMissing Values in Features Dataset:\")\n",
    "print(features_dataset.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Values in Sales Dataset:\")\n",
    "print(sales_dataset.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing Values in Stores Dataset:\")\n",
    "print(stores_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b1f32-c090-435b-8b6e-2d0341ee25bd",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "### Observations from Missing Values Analysis\n",
    "\n",
    "#### Features Dataset\n",
    "- The `Features` dataset contains missing values in the following columns:\n",
    "  - **MarkDown1–MarkDown5:** These columns have a significant amount of missing data:\n",
    "    - `MarkDown1`: 4158 missing values (~50% of the dataset).\n",
    "    - `MarkDown2`: 5269 missing values (~64% of the dataset).\n",
    "    - `MarkDown3`: 4577 missing values (~56% of the dataset).\n",
    "    - `MarkDown4`: 4726 missing values (~58% of the dataset).\n",
    "    - `MarkDown5`: 4140 missing values (~50% of the dataset).\n",
    "    - These missing values likely indicate weeks when no markdowns were applied, which aligns with the business context of promotions being sporadic.\n",
    "  - **CPI and Unemployment:** These columns have 585 missing values each (~7% of the dataset).\n",
    "    - Missing values may result from unavailable macroeconomic data for certain periods or regions.\n",
    "\n",
    "#### Sales Dataset\n",
    "- The `Sales` dataset has no missing values. This ensures it is ready for merging and analysis without additional preprocessing.\n",
    "\n",
    "#### Stores Dataset\n",
    "- The `Stores` dataset is complete, with no missing values. This makes it straightforward to join with the other datasets.\n",
    "\n",
    "### Implications of Missing Values\n",
    "1. **MarkDown Columns:**\n",
    "   - Missing values could represent a lack of markdown activity during those weeks.\n",
    "   - Filling these values with `0` would be appropriate, as it assumes no markdowns were applied where data is missing.\n",
    "\n",
    "2. **CPI and Unemployment:**\n",
    "   - These macroeconomic factors are crucial for analysis. Since the proportion of missing values is small (~7%), we can use imputation techniques such as:\n",
    "     - Forward-fill or backward-fill: To carry the most recent valid value forward/backward in time.\n",
    "     - Mean or median imputation: To replace missing values with the overall average or median.\n",
    "\n",
    "3. **Next Steps:**\n",
    "   - Decide on the best imputation strategy for `MarkDown1–MarkDown5`, `CPI`, and `Unemployment` based on the business context and analysis goals.\n",
    "   - Proceed with merging the datasets after handling missing values to ensure a clean and complete dataset for analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14aa1433-bc8f-45d6-9e9d-5d2b7d55197a",
   "metadata": {},
   "source": [
    "## Handling Missing Values in Features Dataset\n",
    "\n",
    "### Changes Made\n",
    "1. **MarkDown1–MarkDown5:**\n",
    "   - These columns represent promotional markdowns. Missing values are assumed to indicate no markdown activity during those weeks.\n",
    "   - Missing values are replaced with `0` to reflect this assumption\n",
    "\n",
    "2. **CPI and Unemployment:**\n",
    "   - Forward-fill (`ffill()`) was used to propagate the most recent valid value, ensuring temporal continuity for these macroeconomic indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf40cd79-436c-40a3-b5cf-6ca17720c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store           0\n",
      "Date            0\n",
      "Temperature     0\n",
      "Fuel_Price      0\n",
      "MarkDown1       0\n",
      "MarkDown2       0\n",
      "MarkDown3       0\n",
      "MarkDown4       0\n",
      "MarkDown5       0\n",
      "CPI             0\n",
      "Unemployment    0\n",
      "IsHoliday       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values in MarkDown1–MarkDown5 by replacing with 0\n",
    "features_dataset[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']] = features_dataset[['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']].fillna(0)\n",
    "\n",
    "# Handle missing values in CPI and Unemployment using forward-fill\n",
    "features_dataset['CPI'] = features_dataset['CPI'].ffill()\n",
    "features_dataset['Unemployment'] = features_dataset['Unemployment'].ffill()\n",
    "\n",
    "# Verify missing values have been handled\n",
    "print(features_dataset.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a86dab4-390e-4d3b-8edf-7476b283e059",
   "metadata": {},
   "source": [
    "## Converting Date Columns to Datetime Format\n",
    "\n",
    "### Why Convert to Datetime Format?\n",
    "1. **Consistency:** Ensures dates in the `Features` and `Sales` datasets have a consistent format for easier merging and analysis.\n",
    "2. **Time-Series Operations:** Enables operations like extracting months, weeks, or years for deeper analysis of trends.\n",
    "3. **Visualization:** Simplifies plotting metrics over time, such as sales trends.\n",
    "\n",
    "### Code Implementation\n",
    "- We use `pd.to_datetime()` from `pandas` to convert the `Date` columns to datetime format.\n",
    "- The format provided (`%d/%m/%Y`) matches the dataset structure (`day/month/year`).\n",
    "\n",
    "### Results\n",
    "- The `Date` columns in both datasets are now in proper datetime format.\n",
    "- This allows us to perform time-based operations seamlessly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5986b83-04ff-48c5-821d-cd332b04f47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   2010-02-05\n",
      "1   2010-02-12\n",
      "2   2010-02-19\n",
      "3   2010-02-26\n",
      "4   2010-03-05\n",
      "Name: Date, dtype: datetime64[ns]\n",
      "0   2010-02-05\n",
      "1   2010-02-12\n",
      "2   2010-02-19\n",
      "3   2010-02-26\n",
      "4   2010-03-05\n",
      "Name: Date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Date' column in Features dataset to datetime\n",
    "features_dataset['Date'] = pd.to_datetime(features_dataset['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Convert 'Date' column in Sales dataset to datetime\n",
    "sales_dataset['Date'] = pd.to_datetime(sales_dataset['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# Verify the conversion\n",
    "print(features_dataset['Date'].head())\n",
    "print(sales_dataset['Date'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2fa5c1-6c78-428d-9866-01346151b0fc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24a17d5-003d-4c80-9a61-8825b6e4932a",
   "metadata": {},
   "source": [
    "## Import DataSets to MySQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8993a24-2f62-4d7c-bba1-97ff28d426ac",
   "metadata": {},
   "source": [
    "#### DataSets and Database definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56787bb8-63d5-46ff-8509-649ae0970e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_name = \"retail_data\" # Database name\n",
    "tables = [stores_dataset, features_dataset, sales_dataset] # List of DataFrames\n",
    "tables_title = ['stores', 'features', 'sales'] # Name of Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c59401-c692-461a-9357-a6dcd03878ca",
   "metadata": {},
   "source": [
    "#### Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "53d12ba8-c6ef-4c91-b233-cda0c345af55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection open\n"
     ]
    }
   ],
   "source": [
    "cnx = pymysql.connect(user='root', password='root', host='localhost') # Authentication\n",
    "if cnx.open:\n",
    "    print(\"Connection open\")\n",
    "else:\n",
    "    print(\"Connection is not successfully open\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fa1527-ff4f-4d0f-a266-b262b378a2a3",
   "metadata": {},
   "source": [
    "### Creating cursor object to interact with database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2409636e-f0e3-4c50-b6aa-6993d1471cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = cnx.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb382b9-faae-4be4-96fc-277747dd7a55",
   "metadata": {},
   "source": [
    "### Create the Database \"retail_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bce663d2-62f7-46fe-b847-5254d27bc43b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"DROP DATABASE IF EXISTS {database_name}\" # Drop the Database\n",
    "cursor.execute(query)\n",
    "\n",
    "query = f\"CREATE DATABASE IF NOT EXISTS {database_name}\" # Create the Database\n",
    "cursor.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "edccfe9d-00f0-49e6-84b3-67693c1fd36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store_qry = f\"\"\"CREATE TABLE IF NOT EXISTS {database_name}.stores (\n",
    "    Store INT PRIMARY KEY,\n",
    "    Type VARCHAR(50) NOT NULL,\n",
    "    Size INT NOT NULL\n",
    ")\"\"\"\n",
    "cursor.execute(store_qry) # Create Store Table\n",
    "\n",
    "feature_qry = f\"\"\"CREATE TABLE IF NOT EXISTS {database_name}.features (\n",
    "    Store INT,\n",
    "    `Date` DATE NOT NULL,\n",
    "    Temperature FLOAT NOT NULL,\n",
    "    Fuel_Price FLOAT NOT NULL,\n",
    "    MarkDown1 FLOAT NOT NULL DEFAULT 0,\n",
    "    MarkDown2 FLOAT NOT NULL DEFAULT 0,\n",
    "    MarkDown3 FLOAT NOT NULL DEFAULT 0,\n",
    "    MarkDown4 FLOAT NOT NULL DEFAULT 0,\n",
    "    MarkDown5 FLOAT NOT NULL DEFAULT 0,\n",
    "    CPI FLOAT NOT NULL DEFAULT 0,\n",
    "    Unemployment FLOAT NOT NULL DEFAULT 0,\n",
    "    IsHoliday VARCHAR(10),\n",
    "    PRIMARY KEY (Store, Date) \n",
    ")\"\"\"\n",
    "cursor.execute(feature_qry) # Create Feature Table\n",
    "\n",
    "sales_qry = f\"\"\"CREATE TABLE IF NOT EXISTS {database_name}.sales (\n",
    "    Store INT,\n",
    "    Dept INT NOT NULL,\n",
    "    `Date` DATE NOT NULL,\n",
    "    Weekly_Sales FLOAT NOT NULL,\n",
    "    IsHoliday  VARCHAR(10),\n",
    "    PRIMARY KEY (Store, Dept, Date) \n",
    ")\"\"\"\n",
    "cursor.execute(sales_qry) # Create Feature Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "72be144d-48b1-4359-bf93-adeffed6fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting into ... tables_title[index]\n",
      "   All Rows Inserted\n",
      "Inserting into ... tables_title[index]\n",
      "   All Rows Inserted\n",
      "Inserting into ... tables_title[index]\n",
      "   All Rows Inserted\n"
     ]
    }
   ],
   "source": [
    "# Delete all rows in tables before insert\n",
    "cursor.execute(f'delete from {database_name}.stores') # Delete all rows form Stores Table\n",
    "cursor.execute(f'delete from {database_name}.features') # Delete all rows from Features Table\n",
    "cursor.execute(f'delete from {database_name}.sales') # Delete all rows from Sales Table\n",
    "\n",
    "# Insert rows from dataframes into the tables\n",
    "for index, table in enumerate(tables):    ## Loop over tables\n",
    "    columns = table.columns ## Needed to create our query as the insert into takes in the column names and the values as a parameter  \n",
    "    param_1 = \", \".join(list(columns)) ## Creating the string for the list of columns\n",
    "    param_2 = (\"%s, \" * len(columns))[:-2]  ## Creating the placeholder for the value\n",
    "\n",
    "    query = f\"INSERT INTO {database_name}.{tables_title[index]}({param_1}) VALUES ({param_2})\"\n",
    "    print(f'Inserting into ... tables_title[index]')\n",
    "    for i in range(len(table)):\n",
    "        row = list(table.iloc[i])        ## Getting the parameters to be passed which are the values in the row itself\n",
    "        cursor.execute(query, (row))     ## Executing the query and passing the row as argument so that for each table,\n",
    "    print(f'   All Rows Inserted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ba426948-c7b1-4d0c-8e9c-23092011cf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx.commit() # To Store All Pending Inserts into the Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5b675a48-fef5-4612-9deb-c2878a5dd1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((1, 'A', 151315), (2, 'A', 202307), (3, 'B', 37392), (4, 'A', 205863), (5, 'B', 34875), (6, 'A', 202505), (7, 'B', 70713), (8, 'A', 155078), (9, 'B', 125833), (10, 'B', 126512), (11, 'A', 207499), (12, 'B', 112238), (13, 'A', 219622), (14, 'A', 200898), (15, 'B', 123737), (16, 'B', 57197), (17, 'B', 93188), (18, 'B', 120653), (19, 'A', 203819), (20, 'A', 203742), (21, 'B', 140167), (22, 'B', 119557), (23, 'B', 114533), (24, 'A', 203819), (25, 'B', 128107), (26, 'A', 152513), (27, 'A', 204184), (28, 'A', 206302), (29, 'B', 93638), (30, 'C', 42988), (31, 'A', 203750), (32, 'A', 203007), (33, 'A', 39690), (34, 'A', 158114), (35, 'B', 103681), (36, 'A', 39910), (37, 'C', 39910), (38, 'C', 39690), (39, 'A', 184109), (40, 'A', 155083), (41, 'A', 196321), (42, 'C', 39690), (43, 'C', 41062), (44, 'C', 39910), (45, 'B', 118221))\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(f'SELECT * FROM {database_name}.stores') ## Check if rows exists in Store Table\n",
    "store_tbl = cursor.fetchall()\n",
    "\n",
    "print(store_tbl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
